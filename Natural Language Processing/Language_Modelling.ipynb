{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbnmIKF_RzAz"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "# nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlR8OtYwS1in"
      },
      "source": [
        "sentence = \"\"\"The unanimous Declaration of the thirteen united States of America, When in the Course of human events, it becomes necessary for one people to dissolve the political bands which have connected them with another, and to assume among the powers of the earth, the separate and equal station to which the Laws of Nature and of Nature's God entitle them, a decent respect to the opinions of mankind requires that they should declare the causes which impel them to the separation.\n",
        "\n",
        "We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.--That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, --That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness. Prudence, indeed, will dictate that Governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn, that mankind are more disposed to suffer, while evils are sufferable, than to right themselves by abolishing the forms to which they are accustomed. But when a long train of abuses and usurpations, pursuing invariably the same Object evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, to throw off such Government, and to provide new Guards for their future security.--Such has been the patient sufferance of these Colonies; and such is now the necessity which constrains them to alter their former Systems of Government. The history of the present King of Great Britain is a history of repeated injuries and usurpations, all having in direct object the establishment of an absolute Tyranny over these States. To prove this, let Facts be submitted to a candid world.\n",
        "\n",
        "He has refused his Assent to Laws, the most wholesome and necessary for the public good.\n",
        "\n",
        "He has forbidden his Governors to pass Laws of immediate and pressing importance, unless suspended in their operation till his Assent should be obtained; and when so suspended, he has utterly neglected to attend to them.\n",
        "\n",
        "He has refused to pass other Laws for the accommodation of large districts of people, unless those people would relinquish the right of Representation in the Legislature, a right inestimable to them and formidable to tyrants only.\n",
        "\n",
        "He has called together legislative bodies at places unusual, uncomfortable, and distant from the depository of their public Records, for the sole purpose of fatiguing them into compliance with his measures.\n",
        "\n",
        "He has dissolved Representative Houses repeatedly, for opposing with manly firmness his invasions on the rights of the people.\n",
        "\n",
        "He has refused for a long time, after such dissolutions, to cause others to be elected; whereby the Legislative powers, incapable of Annihilation, have returned to the People at large for their exercise; the State remaining in the mean time exposed to all the dangers of invasion from without, and convulsions within.\n",
        "\n",
        "He has endeavoured to prevent the population of these States; for that purpose obstructing the Laws for Naturalization of Foreigners; refusing to pass others to encourage their migrations hither, and raising the conditions of new Appropriations of Lands.\n",
        "\n",
        "He has obstructed the Administration of Justice, by refusing his Assent to Laws for establishing Judiciary powers.\n",
        "\n",
        "He has made Judges dependent on his Will alone, for the tenure of their offices, and the amount and payment of their salaries.\n",
        "\n",
        "He has erected a multitude of New Offices, and sent hither swarms of Officers to harrass our people, and eat out their substance.\n",
        "\n",
        "He has kept among us, in times of peace, Standing Armies without the Consent of our legislatures.\n",
        "\n",
        "He has affected to render the Military independent of and superior to the Civil power.\n",
        "\n",
        "He has combined with others to subject us to a jurisdiction foreign to our constitution, and unacknowledged by our laws; giving his Assent to their Acts of pretended Legislation:\n",
        "\n",
        "For Quartering large bodies of armed troops among us:\n",
        "\n",
        "For protecting them, by a mock Trial, from punishment for any Murders which they should commit on the Inhabitants of these States:\n",
        "\n",
        "For cutting off our Trade with all parts of the world:\n",
        "\n",
        "For imposing Taxes on us without our Consent:\n",
        "\n",
        "For depriving us in many cases, of the benefits of Trial by Jury:\n",
        "\n",
        "For transporting us beyond Seas to be tried for pretended offences\n",
        "\n",
        "For abolishing the free System of English Laws in a neighbouring Province, establishing therein an Arbitrary government, and enlarging its Boundaries so as to render it at once an example and fit instrument for introducing the same absolute rule into these Colonies:\n",
        "\n",
        "For taking away our Charters, abolishing our most valuable Laws, and altering fundamentally the Forms of our Governments:\n",
        "\n",
        "For suspending our own Legislatures, and declaring themselves invested with power to legislate for us in all cases whatsoever.\n",
        "\n",
        "He has abdicated Government here, by declaring us out of his Protection and waging War against us.\n",
        "\n",
        "He has plundered our seas, ravaged our Coasts, burnt our towns, and destroyed the lives of our people.\n",
        "\n",
        "He is at this time transporting large Armies of foreign Mercenaries to compleat the works of death, desolation and tyranny, already begun with circumstances of Cruelty & perfidy scarcely paralleled in the most barbarous ages, and totally unworthy the Head of a civilized nation.\n",
        "\n",
        "He has constrained our fellow Citizens taken Captive on the high Seas to bear Arms against their Country, to become the executioners of their friends and Brethren, or to fall themselves by their Hands.\n",
        "\n",
        "He has excited domestic insurrections amongst us, and has endeavoured to bring on the inhabitants of our frontiers, the merciless Indian Savages, whose known rule of warfare, is an undistinguished destruction of all ages, sexes and conditions.\n",
        "\n",
        "In every stage of these Oppressions We have Petitioned for Redress in the most humble terms: Our repeated Petitions have been answered only by repeated injury. A Prince whose character is thus marked by every act which may define a Tyrant, is unfit to be the ruler of a free people.\n",
        "\n",
        "Nor have We been wanting in attentions to our Brittish brethren. We have warned them from time to time of attempts by their legislature to extend an unwarrantable jurisdiction over us. We have reminded them of the circumstances of our emigration and settlement here. We have appealed to their native justice and magnanimity, and we have conjured them by the ties of our common kindred to disavow these usurpations, which, would inevitably interrupt our connections and correspondence. They too have been deaf to the voice of justice and of consanguinity. We must, therefore, acquiesce in the necessity, which denounces our Separation, and hold them, as we hold the rest of mankind, Enemies in War, in Peace Friends.\n",
        "\n",
        "We, therefore, the Representatives of the united States of America, in General Congress, Assembled, appealing to the Supreme Judge of the world for the rectitude of our intentions, do, in the Name, and by Authority of the good People of these Colonies, solemnly publish and declare, That these United Colonies are, and of Right ought to be Free and Independent States; that they are Absolved from all Allegiance to the British Crown, and that all political connection between them and the State of Great Britain, is and ought to be totally dissolved; and that as Free and Independent States, they have full Power to levy War, conclude Peace, contract Alliances, establish Commerce, and to do all other Acts and Things which Independent States may of right do. And for the support of this Declaration, with a firm reliance on the protection of divine Providence, we mutually pledge to each other our Lives, our Fortunes and our sacred Honor.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjKTFmhuS7_0"
      },
      "source": [
        "import re\n",
        "\n",
        "def clean_text(sentence):\n",
        "  sentence = sentence.lower()\n",
        "  sentence = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
        "  sentence = \" \".join([w for w in sentence.split()])\n",
        "  return sentence\n",
        "\n",
        "sentence = clean_text(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjU4QMs3UIeL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "c43151ae-27c4-47ea-e552-344d6a071061"
      },
      "source": [
        "vocab = list(set([w for w in sentence]))\n",
        "\n",
        "vocab = dict((k, v) for v, k in enumerate(vocab))\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 7,\n",
              " 'a': 2,\n",
              " 'b': 21,\n",
              " 'c': 11,\n",
              " 'd': 25,\n",
              " 'e': 26,\n",
              " 'f': 0,\n",
              " 'g': 17,\n",
              " 'h': 14,\n",
              " 'i': 22,\n",
              " 'j': 6,\n",
              " 'k': 20,\n",
              " 'l': 16,\n",
              " 'm': 5,\n",
              " 'n': 18,\n",
              " 'o': 24,\n",
              " 'p': 8,\n",
              " 'q': 23,\n",
              " 'r': 19,\n",
              " 's': 15,\n",
              " 't': 12,\n",
              " 'u': 9,\n",
              " 'v': 13,\n",
              " 'w': 4,\n",
              " 'x': 3,\n",
              " 'y': 10,\n",
              " 'z': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZhPCb-6U82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61719815-2c07-4616-b74a-7b6d2bf8d896"
      },
      "source": [
        "encode = [vocab[w] for w in sentence]\n",
        "encode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 9,\n",
              " 18,\n",
              " 2,\n",
              " 18,\n",
              " 22,\n",
              " 5,\n",
              " 24,\n",
              " 9,\n",
              " 15,\n",
              " 7,\n",
              " 25,\n",
              " 26,\n",
              " 11,\n",
              " 16,\n",
              " 2,\n",
              " 19,\n",
              " 2,\n",
              " 12,\n",
              " 22,\n",
              " 24,\n",
              " 18,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 22,\n",
              " 19,\n",
              " 12,\n",
              " 26,\n",
              " 26,\n",
              " 18,\n",
              " 7,\n",
              " 9,\n",
              " 18,\n",
              " 22,\n",
              " 12,\n",
              " 26,\n",
              " 25,\n",
              " 7,\n",
              " 15,\n",
              " 12,\n",
              " 2,\n",
              " 12,\n",
              " 26,\n",
              " 15,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 2,\n",
              " 5,\n",
              " 26,\n",
              " 19,\n",
              " 22,\n",
              " 11,\n",
              " 2,\n",
              " 7,\n",
              " 4,\n",
              " 14,\n",
              " 26,\n",
              " 18,\n",
              " 7,\n",
              " 22,\n",
              " 18,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 11,\n",
              " 24,\n",
              " 9,\n",
              " 19,\n",
              " 15,\n",
              " 26,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 14,\n",
              " 9,\n",
              " 5,\n",
              " 2,\n",
              " 18,\n",
              " 7,\n",
              " 26,\n",
              " 13,\n",
              " 26,\n",
              " 18,\n",
              " 12,\n",
              " 15,\n",
              " 7,\n",
              " 22,\n",
              " 12,\n",
              " 7,\n",
              " 21,\n",
              " 26,\n",
              " 11,\n",
              " 24,\n",
              " 5,\n",
              " 26,\n",
              " 15,\n",
              " 7,\n",
              " 18,\n",
              " 26,\n",
              " 11,\n",
              " 26,\n",
              " 15,\n",
              " 15,\n",
              " 2,\n",
              " 19,\n",
              " 10,\n",
              " 7,\n",
              " 0,\n",
              " 24,\n",
              " 19,\n",
              " 7,\n",
              " 24,\n",
              " 18,\n",
              " 26,\n",
              " 7,\n",
              " 8,\n",
              " 26,\n",
              " 24,\n",
              " 8,\n",
              " 16,\n",
              " 26,\n",
              " 7,\n",
              " 12,\n",
              " 24,\n",
              " 7,\n",
              " 25,\n",
              " 22,\n",
              " 15,\n",
              " 15,\n",
              " 24,\n",
              " 16,\n",
              " 13,\n",
              " 26,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 8,\n",
              " 24,\n",
              " 16,\n",
              " 22,\n",
              " 12,\n",
              " 22,\n",
              " 11,\n",
              " 2,\n",
              " 16,\n",
              " 7,\n",
              " 21,\n",
              " 2,\n",
              " 18,\n",
              " 25,\n",
              " 15,\n",
              " 7,\n",
              " 4,\n",
              " 14,\n",
              " 22,\n",
              " 11,\n",
              " 14,\n",
              " 7,\n",
              " 14,\n",
              " 2,\n",
              " 13,\n",
              " 26,\n",
              " 7,\n",
              " 11,\n",
              " 24,\n",
              " 18,\n",
              " 18,\n",
              " 26,\n",
              " 11,\n",
              " 12,\n",
              " 26,\n",
              " 25,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 5,\n",
              " 7,\n",
              " 4,\n",
              " 22,\n",
              " 12,\n",
              " 14,\n",
              " 7,\n",
              " 2,\n",
              " 18,\n",
              " 24,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 19,\n",
              " 7,\n",
              " 2,\n",
              " 18,\n",
              " 25,\n",
              " 7,\n",
              " 12,\n",
              " 24,\n",
              " 7,\n",
              " 2,\n",
              " 15,\n",
              " 15,\n",
              " 9,\n",
              " 5,\n",
              " 26,\n",
              " 7,\n",
              " 2,\n",
              " 5,\n",
              " 24,\n",
              " 18,\n",
              " 17,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 8,\n",
              " 24,\n",
              " 4,\n",
              " 26,\n",
              " 19,\n",
              " 15,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 26,\n",
              " 2,\n",
              " 19,\n",
              " 12,\n",
              " 14,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 15,\n",
              " 26,\n",
              " 8,\n",
              " 2,\n",
              " 19,\n",
              " 2,\n",
              " 12,\n",
              " 26,\n",
              " 7,\n",
              " 2,\n",
              " 18,\n",
              " 25,\n",
              " 7,\n",
              " 26,\n",
              " 23,\n",
              " 9,\n",
              " 2,\n",
              " 16,\n",
              " 7,\n",
              " 15,\n",
              " 12,\n",
              " 2,\n",
              " 12,\n",
              " 22,\n",
              " 24,\n",
              " 18,\n",
              " 7,\n",
              " 12,\n",
              " 24,\n",
              " 7,\n",
              " 4,\n",
              " 14,\n",
              " 22,\n",
              " 11,\n",
              " 14,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 16,\n",
              " 2,\n",
              " 4,\n",
              " 15,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 18,\n",
              " 2,\n",
              " 12,\n",
              " 9,\n",
              " 19,\n",
              " 26,\n",
              " 7,\n",
              " 2,\n",
              " 18,\n",
              " 25,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 18,\n",
              " 2,\n",
              " 12,\n",
              " 9,\n",
              " 19,\n",
              " 26,\n",
              " 7,\n",
              " 15,\n",
              " 7,\n",
              " 17,\n",
              " 24,\n",
              " 25,\n",
              " 7,\n",
              " 26,\n",
              " 18,\n",
              " 12,\n",
              " 22,\n",
              " 12,\n",
              " 16,\n",
              " 26,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 5,\n",
              " 7,\n",
              " 2,\n",
              " 7,\n",
              " 25,\n",
              " 26,\n",
              " 11,\n",
              " 26,\n",
              " 18,\n",
              " 12,\n",
              " 7,\n",
              " 19,\n",
              " 26,\n",
              " 15,\n",
              " 8,\n",
              " 26,\n",
              " 11,\n",
              " 12,\n",
              " 7,\n",
              " 12,\n",
              " 24,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 24,\n",
              " 8,\n",
              " 22,\n",
              " 18,\n",
              " 22,\n",
              " 24,\n",
              " 18,\n",
              " 15,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 5,\n",
              " 2,\n",
              " 18,\n",
              " 20,\n",
              " 22,\n",
              " 18,\n",
              " 25,\n",
              " 7,\n",
              " 19,\n",
              " 26,\n",
              " 23,\n",
              " 9,\n",
              " 22,\n",
              " 19,\n",
              " 26,\n",
              " 15,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 2,\n",
              " 12,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 10,\n",
              " 7,\n",
              " 15,\n",
              " 14,\n",
              " 24,\n",
              " 9,\n",
              " 16,\n",
              " 25,\n",
              " 7,\n",
              " 25,\n",
              " 26,\n",
              " 11,\n",
              " 16,\n",
              " 2,\n",
              " 19,\n",
              " 26,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 11,\n",
              " 2,\n",
              " 9,\n",
              " 15,\n",
              " 26,\n",
              " 15,\n",
              " 7,\n",
              " 4,\n",
              " 14,\n",
              " 22,\n",
              " 11,\n",
              " 14,\n",
              " 7,\n",
              " 22,\n",
              " 5,\n",
              " 8,\n",
              " 26,\n",
              " 16,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 5,\n",
              " 7,\n",
              " 12,\n",
              " 24,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 15,\n",
              " 26,\n",
              " 8,\n",
              " 2,\n",
              " 19,\n",
              " 2,\n",
              " 12,\n",
              " 22,\n",
              " 24,\n",
              " 18,\n",
              " 7,\n",
              " 4,\n",
              " 26,\n",
              " 7,\n",
              " 14,\n",
              " 24,\n",
              " 16,\n",
              " 25,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 15,\n",
              " 26,\n",
              " 7,\n",
              " 12,\n",
              " 19,\n",
              " 9,\n",
              " 12,\n",
              " 14,\n",
              " 15,\n",
              " 7,\n",
              " 12,\n",
              " 24,\n",
              " 7,\n",
              " 21,\n",
              " 26,\n",
              " 7,\n",
              " 15,\n",
              " 26,\n",
              " 16,\n",
              " 0,\n",
              " 7,\n",
              " 26,\n",
              " 13,\n",
              " 22,\n",
              " 25,\n",
              " 26,\n",
              " 18,\n",
              " 12,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 2,\n",
              " 12,\n",
              " 7,\n",
              " 2,\n",
              " 16,\n",
              " 16,\n",
              " 7,\n",
              " 5,\n",
              " 26,\n",
              " 18,\n",
              " 7,\n",
              " 2,\n",
              " 19,\n",
              " 26,\n",
              " 7,\n",
              " 11,\n",
              " 19,\n",
              " 26,\n",
              " 2,\n",
              " 12,\n",
              " 26,\n",
              " 25,\n",
              " 7,\n",
              " 26,\n",
              " 23,\n",
              " 9,\n",
              " 2,\n",
              " 16,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 2,\n",
              " 12,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 10,\n",
              " 7,\n",
              " 2,\n",
              " 19,\n",
              " 26,\n",
              " 7,\n",
              " 26,\n",
              " 18,\n",
              " 25,\n",
              " 24,\n",
              " 4,\n",
              " 26,\n",
              " 25,\n",
              " 7,\n",
              " 21,\n",
              " 10,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 22,\n",
              " 19,\n",
              " 7,\n",
              " 11,\n",
              " 19,\n",
              " 26,\n",
              " 2,\n",
              " 12,\n",
              " 24,\n",
              " 19,\n",
              " 7,\n",
              " 4,\n",
              " 22,\n",
              " 12,\n",
              " 14,\n",
              " 7,\n",
              " 11,\n",
              " 26,\n",
              " 19,\n",
              " 12,\n",
              " 2,\n",
              " 22,\n",
              " 18,\n",
              " 7,\n",
              " 9,\n",
              " 18,\n",
              " 2,\n",
              " 16,\n",
              " 22,\n",
              " 26,\n",
              " 18,\n",
              " 2,\n",
              " 21,\n",
              " 16,\n",
              " 26,\n",
              " 7,\n",
              " 19,\n",
              " 22,\n",
              " 17,\n",
              " 14,\n",
              " 12,\n",
              " 15,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 2,\n",
              " 12,\n",
              " 7,\n",
              " 2,\n",
              " 5,\n",
              " 24,\n",
              " 18,\n",
              " 17,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 15,\n",
              " 26,\n",
              " 7,\n",
              " 2,\n",
              " 19,\n",
              " 26,\n",
              " 7,\n",
              " 16,\n",
              " 22,\n",
              " 0,\n",
              " 26,\n",
              " 7,\n",
              " 16,\n",
              " 22,\n",
              " 21,\n",
              " 26,\n",
              " 19,\n",
              " 12,\n",
              " 10,\n",
              " 7,\n",
              " 2,\n",
              " 18,\n",
              " 25,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 19,\n",
              " 15,\n",
              " 9,\n",
              " 22,\n",
              " 12,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 14,\n",
              " 2,\n",
              " 8,\n",
              " 8,\n",
              " 22,\n",
              " 18,\n",
              " 26,\n",
              " 15,\n",
              " 15,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 2,\n",
              " 12,\n",
              " 7,\n",
              " 12,\n",
              " 24,\n",
              " 7,\n",
              " 15,\n",
              " 26,\n",
              " 11,\n",
              " 9,\n",
              " 19,\n",
              " 26,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 15,\n",
              " 26,\n",
              " 7,\n",
              " 19,\n",
              " 22,\n",
              " 17,\n",
              " 14,\n",
              " 12,\n",
              " 15,\n",
              " 7,\n",
              " 17,\n",
              " 24,\n",
              " 13,\n",
              " 26,\n",
              " 19,\n",
              " 18,\n",
              " 5,\n",
              " 26,\n",
              " 18,\n",
              " 12,\n",
              " 15,\n",
              " 7,\n",
              " 2,\n",
              " 19,\n",
              " 26,\n",
              " 7,\n",
              " 22,\n",
              " 18,\n",
              " 15,\n",
              " 12,\n",
              " 22,\n",
              " 12,\n",
              " 9,\n",
              " 12,\n",
              " 26,\n",
              " 25,\n",
              " 7,\n",
              " 2,\n",
              " 5,\n",
              " 24,\n",
              " 18,\n",
              " 17,\n",
              " 7,\n",
              " 5,\n",
              " 26,\n",
              " 18,\n",
              " 7,\n",
              " 25,\n",
              " 26,\n",
              " 19,\n",
              " 22,\n",
              " 13,\n",
              " 22,\n",
              " 18,\n",
              " 17,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 22,\n",
              " 19,\n",
              " 7,\n",
              " 6,\n",
              " 9,\n",
              " 15,\n",
              " 12,\n",
              " 7,\n",
              " 8,\n",
              " 24,\n",
              " 4,\n",
              " 26,\n",
              " 19,\n",
              " 15,\n",
              " 7,\n",
              " 0,\n",
              " 19,\n",
              " 24,\n",
              " 5,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 11,\n",
              " 24,\n",
              " 18,\n",
              " 15,\n",
              " 26,\n",
              " 18,\n",
              " 12,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 17,\n",
              " 24,\n",
              " 13,\n",
              " 26,\n",
              " 19,\n",
              " 18,\n",
              " 26,\n",
              " 25,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 2,\n",
              " 12,\n",
              " 7,\n",
              " 4,\n",
              " 14,\n",
              " 26,\n",
              " 18,\n",
              " 26,\n",
              " 13,\n",
              " 26,\n",
              " 19,\n",
              " 7,\n",
              " 2,\n",
              " 18,\n",
              " 10,\n",
              " 7,\n",
              " 0,\n",
              " 24,\n",
              " 19,\n",
              " 5,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 17,\n",
              " 24,\n",
              " 13,\n",
              " 26,\n",
              " 19,\n",
              " 18,\n",
              " 5,\n",
              " 26,\n",
              " 18,\n",
              " 12,\n",
              " 7,\n",
              " 21,\n",
              " 26,\n",
              " 11,\n",
              " 24,\n",
              " 5,\n",
              " 26,\n",
              " 15,\n",
              " 7,\n",
              " 25,\n",
              " 26,\n",
              " 15,\n",
              " 12,\n",
              " 19,\n",
              " 9,\n",
              " 11,\n",
              " 12,\n",
              " 22,\n",
              " 13,\n",
              " 26,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 15,\n",
              " 26,\n",
              " 7,\n",
              " 26,\n",
              " 18,\n",
              " 25,\n",
              " 15,\n",
              " 7,\n",
              " 22,\n",
              " 12,\n",
              " 7,\n",
              " 22,\n",
              " 15,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 19,\n",
              " 22,\n",
              " 17,\n",
              " 14,\n",
              " 12,\n",
              " 7,\n",
              " 24,\n",
              " 0,\n",
              " 7,\n",
              " 12,\n",
              " 14,\n",
              " 26,\n",
              " 7,\n",
              " 8,\n",
              " 26,\n",
              " 24,\n",
              " 8,\n",
              " 16,\n",
              " 26,\n",
              " 7,\n",
              " 12,\n",
              " 24,\n",
              " 7,\n",
              " 2,\n",
              " 16,\n",
              " 12,\n",
              " 26,\n",
              " 19,\n",
              " 7,\n",
              " 24,\n",
              " 19,\n",
              " 7,\n",
              " 12,\n",
              " 24,\n",
              " 7,\n",
              " 2,\n",
              " 21,\n",
              " 24,\n",
              " 16,\n",
              " 22,\n",
              " 15,\n",
              " 14,\n",
              " 7,\n",
              " 22,\n",
              " 12,\n",
              " 7,\n",
              " 2,\n",
              " 18,\n",
              " 25,\n",
              " 7,\n",
              " 12,\n",
              " 24,\n",
              " 7,\n",
              " 22,\n",
              " 18,\n",
              " 15,\n",
              " 12,\n",
              " 22,\n",
              " 12,\n",
              " 9,\n",
              " 12,\n",
              " 26,\n",
              " 7,\n",
              " 18,\n",
              " 26,\n",
              " 4,\n",
              " 7,\n",
              " 17,\n",
              " 24,\n",
              " 13,\n",
              " 26,\n",
              " 19,\n",
              " 18,\n",
              " 5,\n",
              " 26,\n",
              " 18,\n",
              " 12,\n",
              " 7,\n",
              " 16,\n",
              " 2,\n",
              " 10,\n",
              " 22,\n",
              " 18,\n",
              " 17,\n",
              " 7,\n",
              " 22,\n",
              " 12,\n",
              " 15,\n",
              " 7,\n",
              " 0,\n",
              " 24,\n",
              " 9,\n",
              " 18,\n",
              " 25,\n",
              " 2,\n",
              " 12,\n",
              " 22,\n",
              " 24,\n",
              " 18,\n",
              " 7,\n",
              " 24,\n",
              " 18,\n",
              " 7,\n",
              " 15,\n",
              " 9,\n",
              " 11,\n",
              " 14,\n",
              " 7,\n",
              " 8,\n",
              " 19,\n",
              " 22,\n",
              " 18,\n",
              " 11,\n",
              " 22,\n",
              " 8,\n",
              " 16,\n",
              " 26,\n",
              " 15,\n",
              " 7,\n",
              " 2,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ElITnu6WMXr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "df5f73f4-6ccd-4866-afd9-b02b488de3bd"
      },
      "source": [
        "sequence = []\n",
        "\n",
        "for i in range(30, len(encode)):\n",
        "  sequence.append(encode[i-30:i+1])\n",
        "sequence\n",
        "\n",
        "sequence = np.array(sequence)\n",
        "x, y = sequence[:, :-1], sequence[:, -1] \n",
        "\n",
        "x, y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[12, 14, 26, ...,  0,  7, 12],\n",
              "        [14, 26,  7, ...,  7, 12, 14],\n",
              "        [26,  7,  9, ..., 12, 14, 26],\n",
              "        ...,\n",
              "        [24,  9, 19, ...,  7, 14, 24],\n",
              "        [ 9, 19,  7, ..., 14, 24, 18],\n",
              "        [19,  7,  0, ..., 24, 18, 24]]), array([14, 26,  7, ..., 18, 24, 19]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXJwQ0fMcNQP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd070df7-78da-40a1-eede-35a6a258bd31"
      },
      "source": [
        "x.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7893, 30), (7893,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0sraKwhaSyb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1712438a-df2d-4016-8d4a-d33840abd6dd"
      },
      "source": [
        "from keras.layers import GRU, Embedding, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJCVxfIEcp_Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "742870cd-20cf-434a-ad16-18bb5942825d"
      },
      "source": [
        "y = to_categorical(y, num_classes=len(vocab))\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3qd96uydR34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "185bd7ca-4bd7-4bb2-a411-224ac6a95c08"
      },
      "source": [
        "len(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21nc_dUMc0fL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "02700cd1-5c9d-4385-c4b6-cff4320d2d97"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(len(vocab), 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, dropout=0.1, recurrent_dropout=0.2))\n",
        "model.add(Dense(len(vocab), activation='softmax'))\n",
        "\n",
        "print(model.summary())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 30, 50)            1350      \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 150)               90450     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 27)                4077      \n",
            "=================================================================\n",
            "Total params: 95,877\n",
            "Trainable params: 95,877\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbEDHF-tfJP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "5e3cb755-f89d-4cd4-d6c4-201dddb4717f"
      },
      "source": [
        "callbacks = [EarlyStopping(patience=3, verbose=1), ModelCheckpoint(' best_model.h5', save_best_only=True)]\n",
        "model.compile(metrics=['accuracy'], optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "model.fit(x, y, epochs=20, callbacks=callbacks, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 6314 samples, validate on 1579 samples\n",
            "Epoch 1/20\n",
            "6314/6314 [==============================] - 15s 2ms/step - loss: 2.8066 - accuracy: 0.1924 - val_loss: 2.5557 - val_accuracy: 0.2951\n",
            "Epoch 2/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 2.3834 - accuracy: 0.3046 - val_loss: 2.2586 - val_accuracy: 0.3376\n",
            "Epoch 3/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 2.2366 - accuracy: 0.3351 - val_loss: 2.1599 - val_accuracy: 0.3705\n",
            "Epoch 4/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 2.1496 - accuracy: 0.3621 - val_loss: 2.1037 - val_accuracy: 0.3857\n",
            "Epoch 5/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 2.0641 - accuracy: 0.3858 - val_loss: 2.0204 - val_accuracy: 0.4155\n",
            "Epoch 6/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.9836 - accuracy: 0.4042 - val_loss: 2.0015 - val_accuracy: 0.4085\n",
            "Epoch 7/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.9043 - accuracy: 0.4235 - val_loss: 1.9453 - val_accuracy: 0.4351\n",
            "Epoch 8/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.8521 - accuracy: 0.4381 - val_loss: 1.9293 - val_accuracy: 0.4528\n",
            "Epoch 9/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.7895 - accuracy: 0.4568 - val_loss: 1.9035 - val_accuracy: 0.4566\n",
            "Epoch 10/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.7354 - accuracy: 0.4728 - val_loss: 1.8737 - val_accuracy: 0.4554\n",
            "Epoch 11/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.6773 - accuracy: 0.4846 - val_loss: 1.8675 - val_accuracy: 0.4541\n",
            "Epoch 12/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.6300 - accuracy: 0.5048 - val_loss: 1.8599 - val_accuracy: 0.4560\n",
            "Epoch 13/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.5797 - accuracy: 0.5139 - val_loss: 1.8425 - val_accuracy: 0.4699\n",
            "Epoch 14/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.5333 - accuracy: 0.5296 - val_loss: 1.8424 - val_accuracy: 0.4801\n",
            "Epoch 15/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.4904 - accuracy: 0.5418 - val_loss: 1.8323 - val_accuracy: 0.4807\n",
            "Epoch 16/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.4491 - accuracy: 0.5507 - val_loss: 1.8423 - val_accuracy: 0.4788\n",
            "Epoch 17/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.4376 - accuracy: 0.5581 - val_loss: 1.8373 - val_accuracy: 0.4769\n",
            "Epoch 18/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.3954 - accuracy: 0.5664 - val_loss: 1.8151 - val_accuracy: 0.4756\n",
            "Epoch 19/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.3641 - accuracy: 0.5825 - val_loss: 1.8209 - val_accuracy: 0.4839\n",
            "Epoch 20/20\n",
            "6314/6314 [==============================] - 14s 2ms/step - loss: 1.3222 - accuracy: 0.5771 - val_loss: 1.8133 - val_accuracy: 0.4896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f060a11aa20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOm__XFGljIz"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "def generate_output(model, seed_text, seq_length, text_len):\n",
        "  input_text = seed_text\n",
        "  for _ in range(text_len):\n",
        "    embedded = [vocab[w] for w in input_text.lower()]\n",
        "    x_pred = pad_sequences([embedded], padding='pre', maxlen=seq_length)\n",
        "    out = model.predict_classes(x_pred, verbose=1)\n",
        "    print(out)\n",
        "    for k, v in vocab.items():\n",
        "      if(v == out):\n",
        "        break\n",
        "    input_text += k\n",
        "    print(input_text)\n",
        "  return input_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDre-4l2png-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "68eb80ca-b4f1-4640-a6fc-eb27e8fceb2e"
      },
      "source": [
        "text = \"hahaha\"\n",
        "generate_output(model, text, 30, 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 8ms/step\n",
            "[12]\n",
            "hahahat\n",
            "1/1 [==============================] - 0s 8ms/step\n",
            "[22]\n",
            "hahahati\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[24]\n",
            "hahahatio\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "[18]\n",
            "hahahation\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[15]\n",
            "hahahations\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[7]\n",
            "hahahations \n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[12]\n",
            "hahahations t\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[24]\n",
            "hahahations to\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "[7]\n",
            "hahahations to \n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "[12]\n",
            "hahahations to t\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[14]\n",
            "hahahations to th\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[26]\n",
            "hahahations to the\n",
            "1/1 [==============================] - 0s 8ms/step\n",
            "[7]\n",
            "hahahations to the \n",
            "1/1 [==============================] - 0s 8ms/step\n",
            "[11]\n",
            "hahahations to the c\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "[24]\n",
            "hahahations to the co\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[18]\n",
            "hahahations to the con\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[15]\n",
            "hahahations to the cons\n",
            "1/1 [==============================] - 0s 6ms/step\n",
            "[12]\n",
            "hahahations to the const\n",
            "1/1 [==============================] - 0s 8ms/step\n",
            "[19]\n",
            "hahahations to the constr\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[2]\n",
            "hahahations to the constra\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[12]\n",
            "hahahations to the constrat\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "[22]\n",
            "hahahations to the constrati\n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "[24]\n",
            "hahahations to the constratio\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "[18]\n",
            "hahahations to the constration\n",
            "1/1 [==============================] - 0s 8ms/step\n",
            "[15]\n",
            "hahahations to the constrations\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[7]\n",
            "hahahations to the constrations \n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[12]\n",
            "hahahations to the constrations t\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[24]\n",
            "hahahations to the constrations to\n",
            "1/1 [==============================] - 0s 7ms/step\n",
            "[7]\n",
            "hahahations to the constrations to \n",
            "1/1 [==============================] - 0s 5ms/step\n",
            "[12]\n",
            "hahahations to the constrations to t\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hahahations to the constrations to t'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gj3jZpZsZ3u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42544599-6b74-4f19-9aa4-9b87f4f46e08"
      },
      "source": [
        "x = \"hello world\"\n",
        "if (x.split()[-2:] == [None, None]):\n",
        "  print(\"True\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXwAzDgZlL3W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "bc0fdb85-8cb3-4790-f1e1-32db4cf246d2"
      },
      "source": [
        "!pip install pytorch-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\r\u001b[K     |                              | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |                            | 20kB 31.9MB/s eta 0:00:01\r\u001b[K     |                          | 30kB 36.2MB/s eta 0:00:01\r\u001b[K     |                        | 40kB 18.3MB/s eta 0:00:01\r\u001b[K     |                      | 51kB 14.0MB/s eta 0:00:01\r\u001b[K     |                    | 61kB 12.1MB/s eta 0:00:01\r\u001b[K     |                   | 71kB 12.1MB/s eta 0:00:01\r\u001b[K     |                 | 81kB 10.9MB/s eta 0:00:01\r\u001b[K     |               | 92kB 11.8MB/s eta 0:00:01\r\u001b[K     |             | 102kB 11.8MB/s eta 0:00:01\r\u001b[K     |           | 112kB 11.8MB/s eta 0:00:01\r\u001b[K     |         | 122kB 11.8MB/s eta 0:00:01\r\u001b[K     |       | 133kB 11.8MB/s eta 0:00:01\r\u001b[K     |      | 143kB 11.8MB/s eta 0:00:01\r\u001b[K     |    | 153kB 11.8MB/s eta 0:00:01\r\u001b[K     |  | 163kB 11.8MB/s eta 0:00:01\r\u001b[K     || 174kB 11.8MB/s eta 0:00:01\r\u001b[K     || 184kB 11.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\r\u001b[K     |                               | 10kB 31.2MB/s eta 0:00:01\r\u001b[K     |                               | 20kB 29.8MB/s eta 0:00:01\r\u001b[K     |                              | 30kB 12.4MB/s eta 0:00:01\r\u001b[K     |                              | 40kB 8.8MB/s eta 0:00:01\r\u001b[K     |                              | 51kB 8.9MB/s eta 0:00:01\r\u001b[K     |                             | 61kB 10.4MB/s eta 0:00:01\r\u001b[K     |                             | 71kB 9.3MB/s eta 0:00:01\r\u001b[K     |                             | 81kB 10.4MB/s eta 0:00:01\r\u001b[K     |                            | 92kB 10.9MB/s eta 0:00:01\r\u001b[K     |                            | 102kB 10.7MB/s eta 0:00:01\r\u001b[K     |                            | 112kB 10.7MB/s eta 0:00:01\r\u001b[K     |                           | 122kB 10.7MB/s eta 0:00:01\r\u001b[K     |                           | 133kB 10.7MB/s eta 0:00:01\r\u001b[K     |                          | 143kB 10.7MB/s eta 0:00:01\r\u001b[K     |                          | 153kB 10.7MB/s eta 0:00:01\r\u001b[K     |                          | 163kB 10.7MB/s eta 0:00:01\r\u001b[K     |                         | 174kB 10.7MB/s eta 0:00:01\r\u001b[K     |                         | 184kB 10.7MB/s eta 0:00:01\r\u001b[K     |                         | 194kB 10.7MB/s eta 0:00:01\r\u001b[K     |                        | 204kB 10.7MB/s eta 0:00:01\r\u001b[K     |                        | 215kB 10.7MB/s eta 0:00:01\r\u001b[K     |                       | 225kB 10.7MB/s eta 0:00:01\r\u001b[K     |                       | 235kB 10.7MB/s eta 0:00:01\r\u001b[K     |                       | 245kB 10.7MB/s eta 0:00:01\r\u001b[K     |                      | 256kB 10.7MB/s eta 0:00:01\r\u001b[K     |                      | 266kB 10.7MB/s eta 0:00:01\r\u001b[K     |                      | 276kB 10.7MB/s eta 0:00:01\r\u001b[K     |                     | 286kB 10.7MB/s eta 0:00:01\r\u001b[K     |                     | 296kB 10.7MB/s eta 0:00:01\r\u001b[K     |                    | 307kB 10.7MB/s eta 0:00:01\r\u001b[K     |                    | 317kB 10.7MB/s eta 0:00:01\r\u001b[K     |                    | 327kB 10.7MB/s eta 0:00:01\r\u001b[K     |                   | 337kB 10.7MB/s eta 0:00:01\r\u001b[K     |                   | 348kB 10.7MB/s eta 0:00:01\r\u001b[K     |                   | 358kB 10.7MB/s eta 0:00:01\r\u001b[K     |                  | 368kB 10.7MB/s eta 0:00:01\r\u001b[K     |                  | 378kB 10.7MB/s eta 0:00:01\r\u001b[K     |                  | 389kB 10.7MB/s eta 0:00:01\r\u001b[K     |                 | 399kB 10.7MB/s eta 0:00:01\r\u001b[K     |                 | 409kB 10.7MB/s eta 0:00:01\r\u001b[K     |                | 419kB 10.7MB/s eta 0:00:01\r\u001b[K     |                | 430kB 10.7MB/s eta 0:00:01\r\u001b[K     |                | 440kB 10.7MB/s eta 0:00:01\r\u001b[K     |               | 450kB 10.7MB/s eta 0:00:01\r\u001b[K     |               | 460kB 10.7MB/s eta 0:00:01\r\u001b[K     |               | 471kB 10.7MB/s eta 0:00:01\r\u001b[K     |              | 481kB 10.7MB/s eta 0:00:01\r\u001b[K     |              | 491kB 10.7MB/s eta 0:00:01\r\u001b[K     |             | 501kB 10.7MB/s eta 0:00:01\r\u001b[K     |             | 512kB 10.7MB/s eta 0:00:01\r\u001b[K     |             | 522kB 10.7MB/s eta 0:00:01\r\u001b[K     |            | 532kB 10.7MB/s eta 0:00:01\r\u001b[K     |            | 542kB 10.7MB/s eta 0:00:01\r\u001b[K     |            | 552kB 10.7MB/s eta 0:00:01\r\u001b[K     |           | 563kB 10.7MB/s eta 0:00:01\r\u001b[K     |           | 573kB 10.7MB/s eta 0:00:01\r\u001b[K     |          | 583kB 10.7MB/s eta 0:00:01\r\u001b[K     |          | 593kB 10.7MB/s eta 0:00:01\r\u001b[K     |          | 604kB 10.7MB/s eta 0:00:01\r\u001b[K     |         | 614kB 10.7MB/s eta 0:00:01\r\u001b[K     |         | 624kB 10.7MB/s eta 0:00:01\r\u001b[K     |         | 634kB 10.7MB/s eta 0:00:01\r\u001b[K     |        | 645kB 10.7MB/s eta 0:00:01\r\u001b[K     |        | 655kB 10.7MB/s eta 0:00:01\r\u001b[K     |        | 665kB 10.7MB/s eta 0:00:01\r\u001b[K     |       | 675kB 10.7MB/s eta 0:00:01\r\u001b[K     |       | 686kB 10.7MB/s eta 0:00:01\r\u001b[K     |      | 696kB 10.7MB/s eta 0:00:01\r\u001b[K     |      | 706kB 10.7MB/s eta 0:00:01\r\u001b[K     |      | 716kB 10.7MB/s eta 0:00:01\r\u001b[K     |     | 727kB 10.7MB/s eta 0:00:01\r\u001b[K     |     | 737kB 10.7MB/s eta 0:00:01\r\u001b[K     |     | 747kB 10.7MB/s eta 0:00:01\r\u001b[K     |    | 757kB 10.7MB/s eta 0:00:01\r\u001b[K     |    | 768kB 10.7MB/s eta 0:00:01\r\u001b[K     |   | 778kB 10.7MB/s eta 0:00:01\r\u001b[K     |   | 788kB 10.7MB/s eta 0:00:01\r\u001b[K     |   | 798kB 10.7MB/s eta 0:00:01\r\u001b[K     |  | 808kB 10.7MB/s eta 0:00:01\r\u001b[K     |  | 819kB 10.7MB/s eta 0:00:01\r\u001b[K     |  | 829kB 10.7MB/s eta 0:00:01\r\u001b[K     | | 839kB 10.7MB/s eta 0:00:01\r\u001b[K     | | 849kB 10.7MB/s eta 0:00:01\r\u001b[K     || 860kB 10.7MB/s eta 0:00:01\r\u001b[K     || 870kB 10.7MB/s eta 0:00:01\r\u001b[K     || 880kB 10.7MB/s eta 0:00:01\r\u001b[K     || 890kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.13.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     || 1.1MB 50.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.15.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.16.13)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->pytorch-transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->pytorch-transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=6bcbcda30262fa40e13da66ab73eeb44766daf5556352327e7803167403966f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.43 sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1NvWzpEtf5g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6ba92f8-053f-4165-e6a1-ec1f5d05988f"
      },
      "source": [
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "\n",
        "# Encode a text inputs\n",
        "text = \"What is the fastest car\"\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "\n",
        "# Convert indexed tokens in a PyTorch tensor\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the model in evaluation mode to deactivate the DropOut modules\n",
        "model.eval()\n",
        "\n",
        "# If you have a GPU, put everything on cuda\n",
        "tokens_tensor = tokens_tensor.to('cuda')\n",
        "model.to('cuda')\n",
        "\n",
        "# Predict all tokens\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor)\n",
        "    predictions = outputs[0]\n",
        "\n",
        "# Get the predicted next sub-word\n",
        "predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
        "predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "\n",
        "# Print the predicted word\n",
        "print(predicted_text)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " What is the fastest car in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INyIanOssqp8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}