# -*- coding: utf-8 -*-
"""Machine_Translation_spanish_to_english.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n7JxvQKvvfmZD3ly_u9jefPwYafEjK-t
"""

!pip install unidecode
import pandas as pd
import numpy as np
import re
import unidecode
import warnings

from sklearn.model_selection import train_test_split
from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Input
from keras.models import Model, load_model
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing.sequence import pad_sequences

warnings.filterwarnings('ignore')

def open_file(filename):

  with open(filename, mode='rt', encoding='utf-8') as f:
    text = f.read()
  
  return text

text = open_file('/content/drive/My Drive/ML_Data/spa.txt')
text

data = [i.split('\t') for i in text.strip().split('\n')]
data = np.array(data)

df = pd.DataFrame({"spanish": data[:, 1], "english":data[:, 0]})
df.head()

def clean_text(sentence):
  sentence = sentence.lower()
  sentence = unidecode.unidecode(sentence)
  sentence = re.sub("'", '', sentence)
  sentence = re.sub('[^a-zA-Z]', ' ', sentence).split()
  sentence = " ".join(sentence)

  return sentence

df["cleaned_spanish"] = df["spanish"].apply(lambda x : clean_text(x))
df["cleaned_english"] = df["english"].apply(lambda x : clean_text(x))

df.head()

spa_tr, spa_tst, eng_tr, eng_tst = train_test_split(df["cleaned_spanish"], df["cleaned_english"], test_size=0.2, random_state=42)

spanish_vocab_freq  = {}
for i in spa_tr:
  for j in i.split():
    try:
      spanish_vocab_freq[j] += 1
    except KeyError:
      spanish_vocab_freq[j] = 1

spanish_vocab_freq

english_vocab_freq = {}

for i in eng_tr:
  for j in i.split():
    try:
      english_vocab_freq[j] += 1
    except KeyError:
      english_vocab_freq[j] = 1

english_vocab_freq

thresold = 2
cnt = 0
tot_cnt = 0
freq = 0
tot_freq = 0

for key, val in english_vocab_freq.items():
  tot_cnt += 1
  tot_freq += val

  if(val < thresold):
    cnt += 1
    freq += 1

print("% of less frequent english words in vocab", cnt/tot_cnt)
print("% of less frequent english words contribution in vocab", freq/tot_freq)

thresold = 2
cnt = 0
tot_cnt = 0
freq = 0
tot_freq = 0

for key, val in spanish_vocab_freq.items():
  tot_cnt += 1
  tot_freq += val

  if(val < thresold):
    cnt += 1
    freq += 1

print("% of less frequent english words in vocab", cnt/tot_cnt)
print("% of less frequent english words contribution in vocab", freq/tot_freq)

spanish_vocab_dic = {}
cnt = 2
for key, val in spanish_vocab_freq.items():
  if(val >= thresold):
    spanish_vocab_dic[key] = cnt
    cnt += 1

spanish_vocab_dic['<pad>'] = 0
spanish_vocab_dic['<unk>'] = 1

english_vocab_dic = {}
cnt = 4
for key, val in english_vocab_freq.items():
  if(val >= thresold):
    english_vocab_dic[key] = cnt
    cnt += 1

english_vocab_dic['<pad>'] = 0
english_vocab_dic['<unk>'] = 1

eng_seq = []
target_vocab = [key for key, val in english_vocab_freq.items()]
target_vocab
for i in eng_tr:
  sentence = []
  for j in i.split():
    if(j not in target_vocab):
      sentence.append(english_vocab_dic['<unk>'])
    elif(english_vocab_freq[j] < thresold):
      sentence.append(english_vocab_dic['<unk>'])
    else:
      sentence.append(english_vocab_dic[j])
  eng_seq.append(sentence)

eng_valid_seq = []
for i in eng_tst:
  sentence = []
  for j in i.split():
    if(j not in target_vocab):
      sentence.append(english_vocab_dic['<unk>'])
    elif(english_vocab_freq[j] < thresold):
      sentence.append(english_vocab_dic['<unk>'])
    else:
      sentence.append(english_vocab_dic[j])
  eng_valid_seq.append(sentence)

spa_seq = []
target_vocab = [key for key, val in spanish_vocab_freq.items()]
target_vocab
for i in spa_tr:
  sentence = []
  for j in i.split():
    if(j not in target_vocab):
      sentence.append(spanish_vocab_dic['<unk>'])
    elif(spanish_vocab_freq[j] < thresold):
      sentence.append(spanish_vocab_dic['<unk>'])
    else:
      sentence.append(spanish_vocab_dic[j])
  spa_seq.append(sentence)
spa_seq

spa_valid_seq = []
for i in spa_tst:
  sentence = []
  for j in i.split():
    if(j not in target_vocab):
      sentence.append(spanish_vocab_dic['<unk>'])
    elif(spanish_vocab_freq[j] < thresold):
      sentence.append(spanish_vocab_dic['<unk>'])
    else:
      sentence.append(spanish_vocab_dic[j])
  spa_valid_seq.append(sentence)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt

# %matplotlib inline

spa_word_len = []
eng_word_len = []

for i in spa_seq:
  spa_word_len.append(len(i))

for j in eng_seq:
  eng_word_len.append(len(j))

length_df = pd.DataFrame({"eng": eng_word_len, "spa": spa_word_len})

length_df.hist(bins=30)
plt.show()

spa_seq = pad_sequences(spa_seq, maxlen=15, truncating='post', padding='post')

eng_seq = pad_sequences(eng_seq, maxlen=15, truncating='post', padding='post')
eng_seq

spa_valid_seq = pad_sequences(spa_valid_seq, maxlen=15, truncating='post', padding='post')
eng_valid_seq = pad_sequences(eng_valid_seq, maxlen=15, truncating='post', padding='post')

english_vocab_dic['<start>'] = 2
english_vocab_dic['<end>'] = 3

y_tr = []

for i in eng_seq:
  temp = np.insert(i, 0, english_vocab_dic['<start>'])
  y_tr.append(temp)
y_tr_eng = []
for i in y_tr:
  if(0 in list(i)):
    temp = np.insert(i, list(i).index(0), english_vocab_dic['<end>'])
  else:
    temp = np.insert(i, len(i), english_vocab_dic['<end>'])
  y_tr_eng.append(temp)

y_tr_eng = np.array(y_tr_eng)

y_val_tr = []

for i in eng_valid_seq:
  temp = np.insert(i, 0, english_vocab_dic['<start>'])
  y_val_tr.append(temp)

y_val_tr_eng = []
for i in y_val_tr:
  if(0 in list(i)):
    temp = np.insert(i, list(i).index(0), english_vocab_dic['<end>'])
  else:
    temp = np.insert(i, len(i), english_vocab_dic['<end>'])
  y_val_tr_eng.append(temp)

y_val_tr_eng = np.array(y_val_tr_eng)

max_spa_len = 15
max_eng_len = 17

latent_dim = 150

embedding_dim = 200

x_voc = len(spanish_vocab_dic)
y_voc = len(english_vocab_dic)

enc_inputs = Input(shape=(max_spa_len, ))
enc_emb = Embedding(x_voc, embedding_dim, trainable=True)(enc_inputs)
enc_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
enc_output, state_h, state_c = enc_lstm(enc_emb)

dec_inputs = Input(shape=(max_eng_len-1, ))
dec_emb = Embedding(y_voc, embedding_dim, trainable=True)
dec_emb_layer = dec_emb(dec_inputs)
dec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
dec_output, dec_h, dec_c = dec_lstm(dec_emb_layer, initial_state = [state_h, state_c])

dec_dense = TimeDistributed(Dense(y_voc, activation='softmax'))
dec_outputs = dec_dense(dec_output)

model = Model([enc_inputs, dec_inputs], dec_outputs)

model.summary()

model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3, min_delta=0.0001)
mc = ModelCheckpoint('best_model_v4.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)

history = model.fit([spa_seq, y_tr_eng[:, :-1]], y_tr_eng.reshape(y_tr_eng.shape[0], y_tr_eng.shape[1], 1)[:,1:], batch_size=512, epochs=100, callbacks=[es, mc],  validation_data=([spa_valid_seq, y_val_tr_eng[:, :-1]], y_val_tr_eng.reshape(y_val_tr_eng.shape[0], y_val_tr_eng.shape[1], 1)[:, 1:]))

model = load_model('/content/best_model_v4.h5')
model.save_weights('best_model_weights_v4.h5')

reverse_target_word_index = dict((val, key) for key, val in english_vocab_dic.items())
reverse_source_word_index = dict((val, key) for key, val in spanish_vocab_dic.items())

encoder_model = Model(enc_inputs, [enc_output, state_h, state_c])

decoded_state_h = Input(shape=(latent_dim, ))
decoded_state_c = Input(shape=(latent_dim, ))

decode_input = Input(shape=(None, ))
decode_embedded = dec_emb(decode_input)
decode_output, decode_h, decode_c = dec_lstm(decode_embedded, initial_state=[decoded_state_h, decoded_state_c])

decode_output_2 = dec_dense(decode_output)

decoder_model = Model(
    [decode_input] + [decoded_state_h, decoded_state_c],
    [decode_output_2] + [decode_h, decode_c]
)

def decode_sequence(input_seq):
  en_out, en_h, en_c = encoder_model.predict(input_seq)

  target_seq = np.zeros((1, 1))

  target_seq[0, 0] = english_vocab_dic['<start>']

  stop_condition = False
  decoded_sentence = ''

  while not stop_condition:
    output_token, e_h, e_c = decoder_model.predict([target_seq] + [en_h , en_c])
    sample_token_index = np.argmax(output_token[0, -1, :])
    sample_token = reverse_target_word_index[sample_token_index]
    print(sample_token)
    
    if(sample_token != '<end>'):
      decoded_sentence = decoded_sentence + sample_token + ' '

    if((sample_token == '<end>') or (len(decoded_sentence.split()) >= max_eng_len-1 )):
      stop_condition = True
    
    target_seq = np.zeros((1, 1))
    target_seq[0, 0] = sample_token_index

    eh_h, eh_c = e_h, e_c

  return decoded_sentence

# target = y_val_original[15000:15050].tolist()
source = spa_tst[152:153].tolist()
target = eng_tst[152:153].tolist()

predicted = []

for i in spa_valid_seq[152:153]:
  predicted.append(decode_sequence(i.reshape(1, max_spa_len)))

df=pd.DataFrame({"Source":source,"Target":target,"predicted":predicted})
df

