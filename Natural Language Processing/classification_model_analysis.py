# -*- coding: utf-8 -*-
"""classification_model_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZGS6-bpUqMsQr85KQQ6SwlSBcYvgZe6o
"""

'''
Data Info : Sentiment analysis on twitter data, input is string and output is (positive/negative/neutral), 

Models Used: Keras Tokenizer, LSTM, sklearn LabelEncoder

Accuracy : 64%
'''

import pandas as pd
import numpy as np
import nltk
import re
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

from keras.layers import LSTM, Dense, Dropout, Embedding
from keras.models import Sequential, Model
from keras.preprocessing.text import Tokenizer

from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping, ModelCheckpoint

df = pd.read_csv('/content/drive/My Drive/ML_Data/Sentiment.csv')
df.drop(['id', 'relevant_yn', 'relevant_yn_confidence', 'candidate', 'candidate_confidence', 'sentiment_confidence', 'subject_matter', 'subject_matter_confidence', 'candidate_gold', 'name','relevant_yn_gold','retweet_count', 'sentiment_gold',	'subject_matter_gold', 'tweet_coord', 'tweet_created', 'tweet_id',	'tweet_location',	'user_timezone'], axis=1, inplace=True)


import re
def clean_text(sentence):
  sentence = sentence.split()
  lst = ["RT", "@", "#"]
  sentence = [val for val in sentence if lst[0] not in val]
  sentence = [val for val in sentence if lst[1] not in val]
  sentence = [val for val in sentence if lst[2] not in val]
  sentence = " ".join(sentence)
  return sentence
df['text'] = df['text'].apply(lambda x: clean_text(x))
df.head()

x = df.text
y = df.sentiment

y = y.apply(lambda x: x.lower())

label = LabelEncoder()
y = label.fit_transform(y)

token = Tokenizer()
token.fit_on_texts(x)
total_words = len(token.word_index) + 1

x = token.texts_to_sequences(x)

count = 0
for i in x:
  if len(i) > count:
    count = len(i)

x = pad_sequences(x, maxlen=count, padding='pre')

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
y_train = to_categorical(y_train)

model = Sequential()
model.add(Embedding(input_dim=total_words, output_dim=100, input_length=count))
model.add(LSTM(120))
model.add(Dense(3, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

callbacks = [EarlyStopping(patience=5), ModelCheckpoint(filepath='/content/drive/My Drive/ML_Data/hello.h5',save_best_only=True)]
model.fit(x_train, y_train, validation_split=0.2, epochs=10, callbacks=callbacks)

y_pred = model.predict_classes(x_test)


from sklearn.metrics import accuracy_score, f1_score, classification_report

print(accuracy_score(y_pred, y_test))
print(classification_report(y_pred, y_test))
print(f1_score(y_test, y_pred, average='micro'))

'''
Data Info : Sentiment analysis on twitter data, input is string and output is (positive/negative/neutral), 

Models Used: Lemmatizer, sklearn LabelEncoder(output), keras Tokenizer, MultinomialNB

Accuracy : 52%
'''


lemma = WordNetLemmatizer()

def clean_text(sentence):
  sentence = sentence.split()
  lst = ["RT", "@", "#"]
  sentence = [val for val in sentence if lst[0] not in val]
  sentence = [val for val in sentence if lst[1] not in val]
  sentence = [val for val in sentence if lst[2] not in val]
  sentence = [lemma.lemmatize(x) for x in sentence if x not in stopwords.words('english')]
  sentence = " ".join(sentence)
  return sentence

df['text'] = df['text'].apply(lambda x: clean_text(x))
df.head()

x = df.text
y = df.sentiment

y = y.apply(lambda x: x.lower())

label = LabelEncoder()
y = label.fit_transform(y)

token = Tokenizer()
token.fit_on_texts(x)
total_words = len(token.word_index) + 1

x = token.texts_to_sequences(x)

count = 0
for i in x:
  if len(i) > count:
    count = len(i)

x = pad_sequences(x, maxlen=count, padding='pre')

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

naive = MultinomialNB()

naive.fit(x_train, y_train)

y_pred = naive.predict(x_test)

from sklearn.metrics import accuracy_score, f1_score, classification_report

print(accuracy_score(y_pred, y_test))
print(classification_report(y_pred, y_test))
print(f1_score(y_test, y_pred, average='micro'))

'''
Data Info : Sentiment analysis on twitter data, input is string and output is (positive/negative/neutral), 

Models Used: Lemmatizer, sklearn LabelEncoder, TfidfVectorizer, MultinomialNB, stopwords removed
              no punctuation removed.

Accuracy : 63.85%
'''

lemma = WordNetLemmatizer()

def clean_text(sentence):
  sentence = sentence.split()
  lst = ["RT", "@", "#"]
  sentence = [val for val in sentence if lst[0] not in val]
  sentence = [val for val in sentence if lst[1] not in val]
  sentence = [val for val in sentence if lst[2] not in val]
  sentence = [lemma.lemmatize(x) for x in sentence if x not in stopwords.words('english')]
  sentence = " ".join(sentence)
  return sentence

df['text'] = df['text'].apply(lambda x: clean_text(x))
df.head()

x = df.text
y = df.sentiment

y = y.apply(lambda x: x.lower())

label = LabelEncoder()
y = label.fit_transform(y)

token = Tokenizer()
token.fit_on_texts(x)
total_words = len(token.word_index) + 1

word_tfidf = TfidfVectorizer(max_features = 14000)
x = word_tfidf.fit_transform(x)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

naive = MultinomialNB()

naive.fit(x_train, y_train)

y_pred = naive.predict(x_test)

from sklearn.metrics import accuracy_score, f1_score, classification_report

print(accuracy_score(y_pred, y_test))
print(classification_report(y_pred, y_test))
print(f1_score(y_test, y_pred, average='micro'))

'''
Data Info : Sentiment analysis on twitter data, input is string and output is (positive/negative/neutral), 

Models Used: sklearn LabelEncoder, TfidfVectorizer, MultinomialNB, punctuation removed, no stopwords removed, 
             no lemmatier
Accuracy : 63.71%
'''


def clean_text(sentence):
  val = str.maketrans('', '', string.punctuation)
  sentence = [w.translate(val) for w in sentence]
  "".join(sentence)
  return sentence


df['text'].apply(lambda x : clean_text(x))

x = df['text']
y = df['sentiment']

word_tfidf = TfidfVectorizer()

x = word_tfidf.fit_transform(x)

label = LabelEncoder()
y = label.fit_transform(y)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

from sklearn.naive_bayes import MultinomialNB

mnb = MultinomialNB()


mnb.fit(x_train, y_train)

y_pred = mnb.predict(x_test)

from sklearn.metrics import f1_score, classification_report, accuracy_score

print(accuracy_score(y_pred, y_test))
print(f1_score(y_pred, y_test, average='micro'))
print(classification_report(y_pred, y_test))

'''
Data Info : Sentiment analysis on twitter data, input is string and output is (positive/negative/neutral), 

Models Used: sklearn LabelEncoder, TfidfVectorizer, MultinomialNB, punctuation removed, Lemmatizer, no stopwords removed

Accuracy : 64.68%
'''


lemma = WordNetLemmatizer()

def clean_text(sentence):
  val = str.maketrans('', '', string.punctuation)
  sentence = [w.translate(val) for w in sentence]
  "".join(sentence)
  sentence = [lemma.lemmatize(w) for w in sentence]
  " ".join(sentence)
  return sentence


df['text'].apply(lambda x : clean_text(x))

x = df['text']
y = df['sentiment']

word_tfidf = TfidfVectorizer()

x = word_tfidf.fit_transform(x)

label = LabelEncoder()
y = label.fit_transform(y)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

from sklearn.naive_bayes import MultinomialNB

mnb = MultinomialNB()


mnb.fit(x_train, y_train)

y_pred = mnb.predict(x_test)

from sklearn.metrics import f1_score, classification_report, accuracy_score

print(accuracy_score(y_pred, y_test))
print(f1_score(y_pred, y_test, average='micro'))
print(classification_report(y_pred, y_test))

'''
Data Info : Sentiment analysis on twitter data, input is string and output is (positive/negative/neutral), 

Models Used: sklearn LabelEncoder, TfidfVectorizer, MultinomialNB, punctuation removed, Lemmatizer, stopwords removed

Accuracy : 64.82%
'''

import pandas as pd
import numpy as np
import nltk
nltk.download('wordnet')
nltk.download('stopwords')
import re
import string

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical

from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords

df = pd.read_csv('/content/drive/My Drive/ML_Data/Sentiment.csv')
df.drop(['id', 'relevant_yn', 'relevant_yn_confidence', 'candidate', 'candidate_confidence', 'sentiment_confidence', 'subject_matter', 'subject_matter_confidence', 'candidate_gold', 'name','relevant_yn_gold','retweet_count', 'sentiment_gold',	'subject_matter_gold', 'tweet_coord', 'tweet_created', 'tweet_id',	'tweet_location',	'user_timezone'], axis=1, inplace=True)

df.head()

lemma = WordNetLemmatizer()

def clean_text(sentence):
  val = str.maketrans('', '', string.punctuation)
  sentence = [w.translate(val) for w in sentence]
  sentence = "".join(sentence)
  sentence = [lemma.lemmatize(w) for w in sentence.split(" ") if w not in stopwords.words('english')]
  " ".join(sentence)
  return sentence

df['text'].apply(lambda x : clean_text(x))

x = df['text']
y = df['sentiment']

word_tfidf = TfidfVectorizer()

x = word_tfidf.fit_transform(x)

label = LabelEncoder()
y = label.fit_transform(y)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

from sklearn.naive_bayes import MultinomialNB

mnb = MultinomialNB()

mnb.fit(x_train, y_train)

y_pred = mnb.predict(x_test)

from sklearn.metrics import f1_score, classification_report, accuracy_score

print(accuracy_score(y_pred, y_test))
print(f1_score(y_pred, y_test, average='micro'))
print(classification_report(y_pred, y_test))

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")

from sklearn import model_selection, naive_bayes, svm, metrics, preprocessing, linear_model
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn import decomposition, ensemble

from keras.preprocessing.sequence import pad_sequences


df = pd.read_csv('/content/drive/My Drive/ML_Data/Sentiment.csv')
df.drop(['id', 'relevant_yn', 'relevant_yn_confidence', 'candidate', 'candidate_confidence', 'sentiment_confidence', 'subject_matter', 'subject_matter_confidence', 'candidate_gold', 'name','relevant_yn_gold','retweet_count', 'sentiment_gold',	'subject_matter_gold', 'tweet_coord', 'tweet_created', 'tweet_id',	'tweet_location',	'user_timezone'], axis=1, inplace=True)



def clean_data(sentence):
  sentence = [w for w in sentence.split(" ") if "RT" not in w]
  sentence = [w for w in sentence if ":" not in w]
  sentence = [w for w in sentence if "#" not in w]
  sentence = [w for w in sentence if "@" not in w]
  sentence = " ".join(sentence)
  return sentence

df["text"] = df["text"].apply(lambda x: clean_data(x))

x = df.text
y = df.sentiment

label = preprocessing.LabelEncoder()

y = label.fit_transform(y)

# Encoding the input text

count_label = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}')

x_count = count_label.fit_transform(x)


tfidf_word_label = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_features=5000)

x_tfidf_word = tfidf_word_label.fit_transform(x)


tfidf_ngram_word_label = TfidfVectorizer(analyzer='word', ngram_range=(2, 3), token_pattern=r'\w{1,}', max_features=5000)

x_ngram_tfidf_word = tfidf_ngram_word_label.fit_transform(x)


tfidf_char_label = TfidfVectorizer(analyzer='char', token_pattern=r'\w{1,}', max_features=5000)

x_tfidf_char = tfidf_char_label.fit_transform(x)


tfidf_ngram_char_label = TfidfVectorizer(analyzer='char', ngram_range=(2, 3), token_pattern=r'\w{1,}', max_features=5000)

x_ngram_tfidf_char = tfidf_ngram_char_label.fit_transform(x)

x_train_count, x_test_count, y_train, y_test = train_test_split(x_count, y, test_size=0.2, random_state=42)
x_train_tfidf_word, x_test_tfidf_word, y_train, y_test = train_test_split(x_tfidf_word, y, test_size=0.2, random_state=42)
x_train_ngram_tfidf_word, x_test_ngram_tfidf_word, y_train, y_test = train_test_split(x_ngram_tfidf_word, y, test_size=0.2, random_state=42)
x_train_tfidf_char,  x_test_tfidf_char, y_train, y_test = train_test_split(x_tfidf_char, y, test_size=0.2, random_state=42)
x_train_ngram_tfidf_char,  x_test_ngram_tfidf_char, y_train, y_test = train_test_split(x_ngram_tfidf_char, y, test_size=0.2, random_state=42)

def model_training(model, x_train, y_train, x_test, y_test):

  model.fit(x_train, y_train)
  y_pred = model.predict(x_test)
  accuracy = metrics.accuracy_score(y_pred, y_test)

  return accuracy

accuracyAnalysis = {}
accuracy = model_training(naive_bayes.MultinomialNB(), x_train_count, y_train, x_test_count, y_test)
print("NB countVectorizer: ", accuracy)
accuracyAnalysis["NB countVectorizer"] = accuracy

accuracy = model_training(naive_bayes.MultinomialNB(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)
print("NB Tfidf word: ", accuracy)
accuracyAnalysis["NB Tfidf word"] = accuracy

accuracy = model_training(naive_bayes.MultinomialNB(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)
print("NB ngram countVectorizer: ", accuracy)
accuracyAnalysis["NB ngram countVectorizer"] = accuracy

accuracy = model_training(naive_bayes.MultinomialNB(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)
print("NB Tfidf char: ", accuracy)
accuracyAnalysis["NB Tfidf char"] = accuracy

accuracy = model_training(naive_bayes.MultinomialNB(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)
print("NB ngram Tfidf char: ", accuracy)
accuracyAnalysis["NB ngram Tfidf char"] = accuracy

accuracy = model_training(svm.SVC(), x_train_count, y_train, x_test_count, y_test)
print("SVM countVectorizer: ", accuracy)
accuracyAnalysis["SVM countVectorizer"] = accuracy

accuracy = model_training(svm.SVC(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)
print("SVM Tfidf word: ", accuracy)
accuracyAnalysis["SVM Tfidf word"] = accuracy

accuracy = model_training(svm.SVC(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)
print("SVM ngram countVectorizer: ", accuracy)
accuracyAnalysis["SVM ngram countVectorizer"] = accuracy

accuracy = model_training(svm.SVC(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)
print("SVM Tfidf char: ", accuracy)
accuracyAnalysis["SVM Tfidf char"] = accuracy

accuracy = model_training(svm.SVC(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)
print("SVM ngram Tfidf char: ", accuracy)
accuracyAnalysis["SVM ngram Tfidf char"] = accuracy

accuracy = model_training(linear_model.LogisticRegression(), x_train_count, y_train, x_test_count, y_test)
print("LR countVectorizer: ", accuracy)
accuracyAnalysis["LR countVectorizer"] = accuracy

accuracy = model_training(linear_model.LogisticRegression(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)
print("LR Tfidf word: ", accuracy)
accuracyAnalysis["LR Tfidf word"] = accuracy

accuracy = model_training(linear_model.LogisticRegression(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)
print("LR ngram countVectorizer: ", accuracy)
accuracyAnalysis["LR ngram countVectorizer"] = accuracy

accuracy = model_training(linear_model.LogisticRegression(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)
print("LR Tfidf char: ", accuracy)
accuracyAnalysis["LR Tfidf char"] = accuracy

accuracy = model_training(linear_model.LogisticRegression(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)
print("LR ngram Tfidf char: ", accuracy)
accuracyAnalysis["LR ngram Tfidf char"] = accuracy

accuracy = model_training(ensemble.RandomForestClassifier(), x_train_count, y_train, x_test_count, y_test)
print("RF countVectorizer: ", accuracy)
accuracyAnalysis["RF countVectorizer"] = accuracy

accuracy = model_training(ensemble.RandomForestClassifier(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)
print("RF Tfidf word: ", accuracy)
accuracyAnalysis["RF Tfidf word"] = accuracy

accuracy = model_training(ensemble.RandomForestClassifier(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)
print("RF ngram countVectorizer: ", accuracy)
accuracyAnalysis["RF ngram countVectorizer"] = accuracy

accuracy = model_training(ensemble.RandomForestClassifier(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)
print("RF Tfidf char: ", accuracy)
accuracyAnalysis["RF Tfidf char"] = accuracy

accuracy = model_training(ensemble.RandomForestClassifier(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)
print("RF ngram Tfidf char: ", accuracy)
accuracyAnalysis["RF ngram Tfidf char"] = accuracy

accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_count, y_train, x_test_count, y_test)
print("AB countVectorizer: ", accuracy)
accuracyAnalysis["AB countVectorizer"] = accuracy

accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)
print("AB Tfidf word: ", accuracy)
accuracyAnalysis["AB Tfidf word"] = accuracy

accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)
print("AB ngram countVectorizer: ", accuracy)
accuracyAnalysis["AB ngram countVectorizer"] = accuracy

accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)
print("AB Tfidf char: ", accuracy)
accuracyAnalysis["AB Tfidf char"] = accuracy

accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)
print("AB ngram Tfidf char: ", accuracy)
accuracyAnalysis["AB ngram Tfidf char"] = accuracy

metrics = sorted(accuracyAnalysis.items(), key=lambda x: x[1], reverse=True)
metrics

import matplotlib.pyplot as plt
!matplotlib inline

x_metrics = []
y_metrics = []

for i in metrics:
  x_metrics.append(i[0])
  y_metrics.append(i[1])


plt.xlabel("Classification Models for text Data")
plt.ylabel("Accuracy Score")
# plt.plot(x_metrics, y_metrics)
plt.bar(x_metrics, y_metrics)
plt.xticks(x_metrics, rotation='vertical')
plt.show()