# -*- coding: utf-8 -*-
"""IMDB_Class_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_1z5gQh_Gml3_1WjeYJI4H52jLQ3ae4E
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from keras.preprocessing.text import Tokenizer
from keras.layers import LSTM, Dense, Dropout, Embedding
from keras.models import Model, Sequential
from keras.preprocessing.sequence import pad_sequences

from keras.datasets import imdb
top_words = 5000
(x_train, y_train), (x_test, y_test) = imdb.load_data(nb_words=top_words)

top_words

x_train = pad_sequences(x_train, maxlen=600)
x_test = pad_sequences(x_test, maxlen=600)
x_train[0]

model = Sequential()

model.add(Embedding(top_words+1, 32, input_length=600))
model.add(LSTM(100))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')

model.summary()

model.fit(x_train, y_train, validation_split=0.2, batch_size=64, epochs=5)