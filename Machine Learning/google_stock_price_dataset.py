# -*- coding: utf-8 -*-
"""Google stock price dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/167xMiBpdksuWhNE3_dFzSSQ4s7ScDKTh
"""

import pandas as pd
 
import numpy as np 

import matplotlib.pyplot as plt

import keras


data = pd.read_csv('/content/Google_Stock_Price_Train.csv')
len(data)

train_data = data.iloc[:,[3]]

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

train_data = sc.fit_transform(train_data)


X_train = []
y_train = []
# creating timestamp for predicting new results
for i in range(60,len(data)):
    X_train.append(train_data[i-60:i,0])
    y_train.append(train_data[i,0])
    
X_train = np.array(X_train)
y_train = np.array(y_train)

# keras way to reshape the input
X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))


from keras.models import Sequential  
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout


regressor = Sequential()

regressor.add(LSTM(units = 50,return_sequences=True,input_shape = (X_train.shape[1],1)))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 50,return_sequences=True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 50, return_sequences=True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units=50))
regressor.add(Dropout(0.2))

regressor.add(Dense(units =1))


regressor.compile(optimizer='adam',loss='mean_squared_error')

regressor.fit(X_train,y_train,epochs=20,batch_size=30)



data_test = pd.read_csv('/content/Google_Stock_Price_Test.csv')

data_test = data_test.iloc[:,[3]]




dataset_total = pd.concat((data['Low'], data_test['Low']), axis = 0)
inputs = dataset_total[len(dataset_total) - len(data_test) - 60:].values
inputs = inputs.reshape(-1,1)
inputs = sc.transform(inputs)
X_test = []
for i in range(60, len(inputs)):
    X_test.append(inputs[i-60:i, 0])
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
predicted_stock_price = regressor.predict(X_test)
predicted_stock_price = sc.inverse_transform(predicted_stock_price)


plt.plot(data_test,color = 'red',label = 'real_google_stock_price')
plt.plot(predicted_stock_price,color = 'green',label = 'predicted_google_stock_price')
plt.title('Google_stock_prices')
plt.xlabel('Time')
plt.ylabel('stock_price')
plt.legend()
plt.show()

data = pd.read_csv('/content/Google_Stock_Price_Train.csv')
print(data.head())

train_data = data.iloc[:,[3]]
print(train_data.shape)
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

train_data = sc.fit_transform(train_data)

X_train = []
y_train = []

# creating timestamp for predicting new results
for i in range(60,len(data)):
    X_train.append(train_data[i-60:i,0])
    y_train.append(train_data[i,0])
    

X_train = np.array(X_train)
y_train = np.array(y_train)

# keras way to reshape the input
X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))

print(X_train.shape)



import pandas as pd
import numpy as np
import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.tokenize import WhitespaceTokenizer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.preprocessing import LabelEncoder


data = pd.read_csv('/content/bbc_news_mixed.csv')

label_encoder = LabelEncoder()

data['label'] = label_encoder.fit_transform(data['label'])

lemma = WordNetLemmatizer()
corpus = []

for sentence in range(len(data['text'])):
    cleanr = re.compile('<.*?>')
    sentence = re.sub(cleanr, ' ', data['text'][sentence])        
    sentence = re.sub(r'[?|!|\'|"|#]',r' ',sentence)
    sentence = re.sub(r'[.|,|)|(|\|/]',r' ',sentence)
    text = word_tokenize(sentence)
    text = [lemma.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]
    text = [val for val in text if len(val) > 2]
    text = " ".join(text)
    corpus.append(text)

corpus

from sklearn.feature_extraction.text import TfidfVectorizer

TF = TfidfVectorizer(max_features = 5000)

vectors = TF.fit_transform(corpus).toarray()

x = pd.DataFrame(vectors)

x.to_sparse

from google.colab import files
import pandas as pd

data_to_load = files.upload()

import pandas as pd
import numpy as np
import re
bbc_news = pd.read_csv('/content/bbc_news_mixed.csv')

from gensim.utils import simple_preprocess


preprocessed_bbc = bbc_news.text.apply(lambda x: simple_preprocess(x))
preprocessed_bbc

from gensim.models import Word2Vec
w2v_model = Word2Vec(preprocessed_bbc, size=300, min_count=2, sg=1)

print('vocabulary size:', len(w2v_model.wv.vocab))

list(w2v_model.wv.vocab.items())[:20]

words_oil = w2v_model.wv.most_similar('oil')
print(words_oil)

words_pc = w2v_model.wv.most_similar('pc')
print(words_pc)

words_music = w2v_model.wv.most_similar('music')
words_football = w2v_model.wv.most_similar('football')
words = words_oil + words_music + words_football
words = list(map(lambda x: x[0], words))
words

from matplotlib import pyplot
from sklearn.decomposition import PCA


def plot_w2v(word_list):
    X = w2v_model[word_list]
   
    pca = PCA(n_components=2)
    result = pca.fit_transform(X)
    
    
    pyplot.scatter(result[:, 0], result[:, 1])
    for i, word in enumerate(word_list):
        pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
        
    
    pyplot.figure(figsize=(6,15))
    pyplot.show()

plot_w2v(words)

def get_embedding_w2v(doc_tokens):
    embeddings = []
    
    model = w2v_model
      
    for tok in doc_tokens:
        if tok in model.wv.vocab:
            embeddings.append(model.wv.word_vec(tok))
    
    return np.mean(embeddings, axis=0)

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

X = preprocessed_bbc.apply(lambda x: get_embedding_w2v(x))
X = pd.DataFrame(X.to_list())

label_encoder = LabelEncoder()

y = label_encoder.fit_transform(bbc_news.label)

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state =0)

from sklearn.naive_bayes import GaussianNB

model = GaussianNB()

model.fit(X_train,y_train)

y_predict = model.predict(X_test)

from sklearn.metrics import confusion_matrix,accuracy_score,classification_report

print(confusion_matrix(y_test,y_predict))

print(accuracy_score(y_test,y_predict))

print(classification_report(y_test,y_predict))

