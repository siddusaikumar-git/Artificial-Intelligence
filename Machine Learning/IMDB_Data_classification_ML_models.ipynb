{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN09lpdo8zXt"
      },
      "source": [
        "#Machine Learning Models on IMDB Classification Data\n",
        "1.   Naive Bayes\n",
        "2.   Support Vector Machines\n",
        "3.   Logistic Regression\n",
        "4.   Random Forest\n",
        "5.   Ada Boost\n",
        "6.   XG Boost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqapIDOkdp9p"
      },
      "source": [
        "*Import Libraries and loading the DataFrame*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfCoiCJHr7Et",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "8728ba57-d732-49a2-dfb2-e09d80818620"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import re\n",
        "import string\n",
        "!pip install contractions\n",
        "import contractions\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn import model_selection, naive_bayes, svm, metrics, preprocessing, linear_model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/ML_Data/IMDB Dataset.csv\")\n",
        "\n",
        "x = df.review\n",
        "y = df.sentiment\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/85/41/c3dfd5feb91a8d587ed1a59f553f07c05f95ad4e5d00ab78702fbf8fe48a/contractions-0.0.24-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 3.7MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 15.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81696 sha256=68e0ab4f719b9c394d65b6fe9a53fc9b3ac725362e0d53865694320b4a54cfbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.24 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQQnt3466G-g"
      },
      "source": [
        "*Noise Removal*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF0BbU-T6EJz"
      },
      "source": [
        "# def strip_html(sentence):\n",
        "#   soup = BeautifulSoup(sentence, \"html.parser\")\n",
        "#   return soup.get_text()\n",
        "\n",
        "\n",
        "# def remove_between_square_brackets(sentence):\n",
        "#     return re.sub('\\[[^]]*\\]', '', sentence)\n",
        "\n",
        "# def denoise_text(sentence):\n",
        "#   sentence = strip_html(sentence)\n",
        "#   sentence = remove_between_square_brackets(sentence)\n",
        "#   return sentence\n",
        "\n",
        "# x = x.apply(lambda val : denoise_text(sentence))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaAykuJO63bf"
      },
      "source": [
        "*Noise Removal and Normalizing the Text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFc8H9qR6_Ga"
      },
      "source": [
        "def normalize_text(sentence):\n",
        "  sentence = sentence.strip()  # removes the starting and ending spaces\n",
        "  sentence = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', sentence) # removes links with http/https\n",
        "  sentence = re.sub(r'[\\[].*\\]', '', sentence) # removes square brackets and text present inside it.\n",
        "  sentence = sentence.translate(str.maketrans('', '', string.punctuation)) # removes punctuation.\n",
        "  sentence = contractions.fix(sentence) # fixes contractions\n",
        "  sentence = sentence.lower() # converts to lower case\n",
        "  return sentence\n",
        "\n",
        "x = x.apply(lambda val : normalize_text(val))\n",
        "\n",
        "label = preprocessing.LabelEncoder()\n",
        "y = label.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of_XoLAUdTOA"
      },
      "source": [
        "*Encoding/Tokenizing the input text after normalization*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0UGPXgepXZZ"
      },
      "source": [
        "\n",
        "count_label = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "x_count = count_label.fit_transform(x)\n",
        "\n",
        "tfidf_word_label = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "x_tfidf_word = tfidf_word_label.fit_transform(x)\n",
        "\n",
        "tfidf_ngram_word_label = TfidfVectorizer(analyzer='word', ngram_range=(2, 3), token_pattern=r'\\w{1,}', max_features=5000)\n",
        "x_ngram_tfidf_word = tfidf_ngram_word_label.fit_transform(x)\n",
        "\n",
        "tfidf_char_label = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "x_tfidf_char = tfidf_char_label.fit_transform(x)\n",
        "\n",
        "tfidf_ngram_char_label = TfidfVectorizer(analyzer='char', ngram_range=(2, 3), token_pattern=r'\\w{1,}', max_features=5000)\n",
        "x_ngram_tfidf_char = tfidf_ngram_char_label.fit_transform(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBj58rxIdytb"
      },
      "source": [
        "*Split Dataset test and train with 20% as test data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIiw4Jj_pYKJ"
      },
      "source": [
        "x_train_count, x_test_count, y_train, y_test = train_test_split(x_count, y, test_size=0.2, random_state=42)\n",
        "x_train_tfidf_word, x_test_tfidf_word, y_train, y_test = train_test_split(x_tfidf_word, y, test_size=0.2, random_state=42)\n",
        "x_train_ngram_tfidf_word, x_test_ngram_tfidf_word, y_train, y_test = train_test_split(x_ngram_tfidf_word, y, test_size=0.2, random_state=42)\n",
        "x_train_tfidf_char,  x_test_tfidf_char, y_train, y_test = train_test_split(x_tfidf_char, y, test_size=0.2, random_state=42)\n",
        "x_train_ngram_tfidf_char,  x_test_ngram_tfidf_char, y_train, y_test = train_test_split(x_ngram_tfidf_char, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d78CrbEujHAC"
      },
      "source": [
        "*Training on Train Data, Predicting Test Data, Calculating Accuracy*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcVV68GApaDS"
      },
      "source": [
        "def model_training(model, x_train, y_train, x_test, y_test):\n",
        "\n",
        "  model.fit(x_train, y_train)\n",
        "  y_pred = model.predict(x_test)\n",
        "  accuracy = metrics.accuracy_score(y_pred, y_test)\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daz8FhBRjdNh"
      },
      "source": [
        "*Naive Bayes Classifier Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y78uoI-7pc7l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e1c3a3fe-69bd-4a9b-b570-55d3e4800de3"
      },
      "source": [
        "accuracyAnalysis = {}\n",
        "accuracy = model_training(naive_bayes.MultinomialNB(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"NB countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"NB countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(naive_bayes.MultinomialNB(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"NB Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"NB Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(naive_bayes.MultinomialNB(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"NB ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"NB ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(naive_bayes.MultinomialNB(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"NB Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"NB Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(naive_bayes.MultinomialNB(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"NB ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"NB ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB countVectorizer:  0.8443\n",
            "NB Tfidf word:  0.8543\n",
            "NB ngram countVectorizer:  0.8403\n",
            "NB Tfidf char:  0.5836\n",
            "NB ngram Tfidf char:  0.8063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxim7PfHjhTe"
      },
      "source": [
        "*Support Vector Machines Model Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC9E-7USpgKa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "219d8d07-82dc-4bde-9a26-c2a28aace270"
      },
      "source": [
        "accuracy = model_training(svm.SVC(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"SVM countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"SVM countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(svm.SVC(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"SVM Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"SVM Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(svm.SVC(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"SVM ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"SVM ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(svm.SVC(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"SVM Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"SVM Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(svm.SVC(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"SVM ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"SVM ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM countVectorizer:  0.88\n",
            "SVM Tfidf word:  0.9008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8cvk2bejq3J"
      },
      "source": [
        "*Logistic Regression Model Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4GDhDJBpjFO"
      },
      "source": [
        "accuracy = model_training(linear_model.LogisticRegression(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"LR countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"LR countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(linear_model.LogisticRegression(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"LR Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"LR Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(linear_model.LogisticRegression(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"LR ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"LR ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(linear_model.LogisticRegression(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"LR Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"LR Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(linear_model.LogisticRegression(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"LR ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"LR ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svQgAmZlj2A5"
      },
      "source": [
        "*Random Forest Classifier Model Analysis*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipjCx0h5plLD"
      },
      "source": [
        "accuracy = model_training(ensemble.RandomForestClassifier(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"RF countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"RF countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.RandomForestClassifier(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"RF Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"RF Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.RandomForestClassifier(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"RF ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"RF ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.RandomForestClassifier(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"RF Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"RF Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.RandomForestClassifier(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"RF ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"RF ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGGvaJH-kAsn"
      },
      "source": [
        "*ADA Boost Classifier Model Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf3uwU9XpnNI"
      },
      "source": [
        "accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"AB countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"AB countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"AB Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"AB Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"AB ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"AB ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"AB Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"AB Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"AB ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"AB ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwcfPSMVkFqk"
      },
      "source": [
        "*XG Boost Classifier Model Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1szBz09C7LER"
      },
      "source": [
        "accuracy = model_training(xgboost.XGBClassifier(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"XGB countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"XGB countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(xgboost.XGBClassifier(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"XGB Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"XGB Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(xgboost.XGBClassifier(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"XGB ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"XGB ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(xgboost.XGBClassifier(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"XGB Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"XGB Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(xgboost.XGBClassifier(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"XGB ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"XGB ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY84EWJrkLXC"
      },
      "source": [
        "*Accuracy Metrics Sorted with Highest at Top*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3-Ukn-Iprde"
      },
      "source": [
        "metrics = sorted(accuracyAnalysis.items(), key=lambda x: x[1], reverse=True)\n",
        "metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAw1VcrakYrU"
      },
      "source": [
        "*Plotting the Accuracy Metrics in Box Plot*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO3Gw8x6uzNQ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "!matplotlib inline\n",
        "\n",
        "x_metrics = []\n",
        "y_metrics = []\n",
        "\n",
        "for i in metrics:\n",
        "  x_metrics.append(i[0])\n",
        "  y_metrics.append(i[1])\n",
        "\n",
        "\n",
        "plt.xlabel(\"Classification Models for text Data\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "# plt.plot(x_metrics, y_metrics)\n",
        "plt.bar(x_metrics, y_metrics)\n",
        "plt.xticks(x_metrics, rotation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcp3-yLdEjZ6"
      },
      "source": [
        "#Lemmatiztion added to Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rw7WqDYgAQZ"
      },
      "source": [
        "*Import Libraries and Loading data to Dataframe*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbBxP5s6Eta7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import nltk\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn import model_selection, naive_bayes, svm, metrics, preprocessing, linear_model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/ML_Data/IMDB Dataset.csv\")\n",
        "\n",
        "\n",
        "x = df['review']\n",
        "y = df['sentiment']\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb3WPYCtgHmx"
      },
      "source": [
        "*Noise Removal and Normalization*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xn9agY7c7gu"
      },
      "source": [
        "def normalize_text(sentence):\n",
        "  sentence = sentence.strip()  # removes the starting and ending spaces\n",
        "  sentence = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', sentence) # removes links with http/https\n",
        "  sentence = re.sub(r'[\\[].*\\]', '', sentence) # removes square brackets and text present inside it.\n",
        "  sentence = sentence.translate(str.maketrans('', '', string.punctuation)) # removes punctuation\n",
        "  sentence = contractions.fix(sentence) # fixes contractions\n",
        "  sentence = sentence.lower() # converts to lower case\n",
        "  sentence = [lemma.lemmatize(w) for w in sentence.split(\" \") if w not in stopwords.words('english')]\n",
        "  sentence = \" \".join(sentence)\n",
        "  return sentence\n",
        "\n",
        "x = x.apply(lambda val: normalize_text(val))\n",
        "\n",
        "label = preprocessing.LabelEncoder()\n",
        "y = label.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfvxGL7UgqV0"
      },
      "source": [
        "*Encoding/Tokenizing the input text after normalization*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qcBTQLDEtbA"
      },
      "source": [
        "\n",
        "count_label = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "x_count = count_label.fit_transform(x)\n",
        "\n",
        "tfidf_word_label = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "x_tfidf_word = tfidf_word_label.fit_transform(x)\n",
        "\n",
        "tfidf_ngram_word_label = TfidfVectorizer(analyzer='word', ngram_range=(2, 3), token_pattern=r'\\w{1,}', max_features=5000)\n",
        "x_ngram_tfidf_word = tfidf_ngram_word_label.fit_transform(x)\n",
        "\n",
        "tfidf_char_label = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "x_tfidf_char = tfidf_char_label.fit_transform(x)\n",
        "\n",
        "tfidf_ngram_char_label = TfidfVectorizer(analyzer='char', ngram_range=(2, 3), token_pattern=r'\\w{1,}', max_features=5000)\n",
        "x_ngram_tfidf_char = tfidf_ngram_char_label.fit_transform(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqpho6Hkg7pd"
      },
      "source": [
        "*Splitting the input data into Train and Test with Test data 20%*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBtjGJwgEtbD"
      },
      "source": [
        "x_train_count, x_test_count, y_train, y_test = train_test_split(x_count, y, test_size=0.2, random_state=42)\n",
        "\n",
        "x_train_tfidf_word, x_test_tfidf_word, y_train, y_test = train_test_split(x_tfidf_word, y, test_size=0.2, random_state=42)\n",
        "\n",
        "x_train_ngram_tfidf_word, x_test_ngram_tfidf_word, y_train, y_test = train_test_split(x_ngram_tfidf_word, y, test_size=0.2, random_state=42)\n",
        "\n",
        "x_train_tfidf_char,  x_test_tfidf_char, y_train, y_test = train_test_split(x_tfidf_char, y, test_size=0.2, random_state=42)\n",
        "\n",
        "x_train_ngram_tfidf_char,  x_test_ngram_tfidf_char, y_train, y_test = train_test_split(x_ngram_tfidf_char, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNddX_c2hNxe"
      },
      "source": [
        "*Training input Train Data, Predicting and calculating accuracy for Test Data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdCivmwyEtbF"
      },
      "source": [
        "def model_training(model, x_train, y_train, x_test, y_test):\n",
        "\n",
        "  model.fit(x_train, y_train)\n",
        "  y_pred = model.predict(x_test)\n",
        "  accuracy = metrics.accuracy_score(y_pred, y_test)\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep0jgh7ghdtz"
      },
      "source": [
        "*Naive Bayes Model Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nj8gN7VEtbL"
      },
      "source": [
        "accuracyAnalysis = {}\n",
        "accuracy = model_training(naive_bayes.MultinomialNB(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"NB countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"NB countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(naive_bayes.MultinomialNB(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"NB Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"NB Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(naive_bayes.MultinomialNB(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"NB ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"NB ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(naive_bayes.MultinomialNB(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"NB Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"NB Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(naive_bayes.MultinomialNB(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"NB ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"NB ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8UYKUJuhoRg"
      },
      "source": [
        "*Support Vector Machine Model Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoOEB6dLEtbO"
      },
      "source": [
        "accuracy = model_training(svm.SVC(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"SVM countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"SVM countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(svm.SVC(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"SVM Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"SVM Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(svm.SVC(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"SVM ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"SVM ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(svm.SVC(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"SVM Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"SVM Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(svm.SVC(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"SVM ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"SVM ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4LQJntthukK"
      },
      "source": [
        "*Logistic Regression Model Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5VZk2e6EtbQ"
      },
      "source": [
        "accuracy = model_training(linear_model.LogisticRegression(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"LR countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"LR countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(linear_model.LogisticRegression(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"LR Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"LR Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(linear_model.LogisticRegression(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"LR ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"LR ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(linear_model.LogisticRegression(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"LR Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"LR Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(linear_model.LogisticRegression(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"LR ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"LR ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfyqlZAEhyvr"
      },
      "source": [
        "*Random Forest Classifier Model Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3V1l1cnEtbS"
      },
      "source": [
        "accuracy = model_training(ensemble.RandomForestClassifier(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"RF countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"RF countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.RandomForestClassifier(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"RF Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"RF Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.RandomForestClassifier(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"RF ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"RF ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.RandomForestClassifier(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"RF Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"RF Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.RandomForestClassifier(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"RF ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"RF ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAeIhRuoh6lM"
      },
      "source": [
        "*ADA Boost Classifier Model Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU315m7yEtbV"
      },
      "source": [
        "accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"AB countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"AB countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"AB Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"AB Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"AB ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"AB ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"AB Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"AB Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(ensemble.AdaBoostClassifier(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"AB ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"AB ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0M0t5xmh_92"
      },
      "source": [
        "*XG Boost Classifier Model Analysis*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "083m77KAEtbX"
      },
      "source": [
        "accuracy = model_training(xgboost.XGBClassifier(), x_train_count, y_train, x_test_count, y_test)\n",
        "print(\"XGB countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"XGB countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(xgboost.XGBClassifier(), x_train_tfidf_word, y_train, x_test_tfidf_word, y_test)\n",
        "print(\"XGB Tfidf word: \", accuracy)\n",
        "accuracyAnalysis[\"XGB Tfidf word\"] = accuracy\n",
        "\n",
        "accuracy = model_training(xgboost.XGBClassifier(), x_train_ngram_tfidf_word, y_train, x_test_ngram_tfidf_word, y_test)\n",
        "print(\"XGB ngram countVectorizer: \", accuracy)\n",
        "accuracyAnalysis[\"XGB ngram countVectorizer\"] = accuracy\n",
        "\n",
        "accuracy = model_training(xgboost.XGBClassifier(), x_train_tfidf_char, y_train, x_test_tfidf_char, y_test)\n",
        "print(\"XGB Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"XGB Tfidf char\"] = accuracy\n",
        "\n",
        "accuracy = model_training(xgboost.XGBClassifier(), x_train_ngram_tfidf_char, y_train, x_test_ngram_tfidf_char, y_test)\n",
        "print(\"XGB ngram Tfidf char: \", accuracy)\n",
        "accuracyAnalysis[\"XGB ngram Tfidf char\"] = accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgrOHan_iFWf"
      },
      "source": [
        "*Accuracy Metrics Ordering with Highest Accuracy at Top*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eGlsnuVEtba"
      },
      "source": [
        "metrics = sorted(accuracyAnalysis.items(), key=lambda x: x[1], reverse=True)\n",
        "metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHhw1sOxiUiB"
      },
      "source": [
        "*Plotting the Accuracy Metrics in Box Plot*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc-l_AZiEtbd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "!matplotlib inline\n",
        "\n",
        "x_metrics_lemmet = []\n",
        "y_metrics_lemmet = []\n",
        "\n",
        "for i in metrics:\n",
        "  x_metrics_lemmet.append(i[0])\n",
        "  y_metrics_lemmet.append(i[1])\n",
        "\n",
        "\n",
        "plt.xlabel(\"Classification Models for text Data With Lemmatization and Stopwords\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "# plt.plot(x_metrics_lemmet, y_metrics_lemmet)\n",
        "plt.bar(x_metrics_lemmet, y_metrics_lemmet)\n",
        "plt.xticks(x_metrics_lemmet, rotation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhTSZWRxHopo"
      },
      "source": [
        "#Accuracy Measures of Models with and without (Lemmatization and Stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLeW4ZpBH2va"
      },
      "source": [
        "plt.xlabel(\"Classification Models for text Data With Lemmatization and Stopwords\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "# plt.plot(x_metrics, y_metrics)\n",
        "plt.bar(x_metrics, y_metrics)\n",
        "plt.xticks(x_metrics, rotation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyvRBwKQHxS-"
      },
      "source": [
        "plt.xlabel(\"Classification Models for text Data With Lemmatization and Stopwords\")\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "# plt.plot(x_metrics_lemmet, y_metrics_lemmet)\n",
        "plt.bar(x_metrics_lemmet, y_metrics_lemmet)\n",
        "plt.xticks(x_metrics_lemmet, rotation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}